import reflex as rx
import os
import time
from pinecone import Pinecone
from openai import OpenAI
from typing import List, Dict, Optional
import fitz
import docx
import uuid
import json
from langchain.text_splitter import RecursiveCharacterTextSplitter


OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.environ.get("PINECONE_INDEX_NAME")
PINECONE_NAMESPACE = os.environ.get("PINECONE_NAMESPACE", "Pruebas")
openai_client_instance: Optional[OpenAI] = None
pinecone_index_instance: Optional[PineconeIndexType] = None
initialization_error: Optional[str] = None

try:
    if OPENAI_API_KEY:
        openai_client_instance = OpenAI(api_key=OPENAI_API_KEY)
    else:
        initialization_error = "OpenAI API key not found. Please set OPENAI_API_KEY."

    if PINECONE_API_KEY and PINECONE_INDEX_NAME:
        pinecone_client = Pinecone(api_key=PINECONE_API_KEY)
        existing_indexes = [idx_spec.name for idx_spec in pinecone_client.list_indexes()]
        if PINECONE_INDEX_NAME in existing_indexes:
            pinecone_index_instance = pinecone_client.Index(PINECONE_INDEX_NAME)
        else:
            error_msg = f"Pinecone index '{PINECONE_INDEX_NAME}' does not exist. Available indexes: {existing_indexes}."
            initialization_error = f"{initialization_error}\n{error_msg}" if initialization_error else error_msg
            print(error_msg)
    elif not PINECONE_API_KEY:
        error_msg = "Pinecone API key not found. Please set PINECONE_API_KEY."
        initialization_error = f"{initialization_error}\n{error_msg}" if initialization_error else error_msg
    elif not PINECONE_INDEX_NAME:
        error_msg = "Pinecone index name not found. Please set PINECONE_INDEX_NAME."
        initialization_error = f"{initialization_error}\n{error_msg}" if initialization_error else error_msg
except Exception as e:
    error_msg = f"Error during client initialization: {e}"
    initialization_error = f"{initialization_error}\n{error_msg}" if initialization_error else error_msg
    print(error_msg)

def get_embedding(pregunta: str) -> Optional[List[float]]:
    if not openai_client_instance:
        print("OpenAI client not initialized.")
        return None
    try:
        response = openai_client_instance.embeddings.create(
            input=pregunta, model="text-embedding-3-small"
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error getting embedding: {e}")
        return None

def buscar_contexto(embedding: List[float], top_k: int = 10) -> str:
    if not pinecone_index_instance:
        print("Pinecone index not initialized.")
        return "Error: Pinecone index not available."
    try:
        query_response = pinecone_index_instance.query(
            vector=embedding,
            top_k=top_k,
            namespace=PINECONE_NAMESPACE,
            include_metadata=True,
        )
        context_parts = []
        for match in query_response.matches:
            if match.metadata and "texto" in match.metadata:
                context_parts.append(match.metadata["texto"])
        return "\n---\n".join(context_parts)
    except Exception as e:
        print(f"Error searching context: {e}")
        return f"Error searching context: {e}"

def generar_respuesta_openai(pregunta: str, contexto: str) -> str:
    if not openai_client_instance:
        print("OpenAI client not initialized.")
        return "Error: OpenAI client not available."
    system_prompt = "Responde exclusivamente con la informaci贸n proporcionada en el contexto. No agregues conocimientos previos ni informaci贸n externa. Si no puedes responder con el contexto, di que no hay suficiente informaci贸n."
    user_prompt = f"{contexto}\n\nPregunta: {pregunta}"
    messages: List[Dict[str, str]] = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    try:
        response = openai_client_instance.chat.completions.create(
            model="gpt-3.5-turbo", messages=messages, temperature=0,
        )
        if response.choices and response.choices[0].message and response.choices[0].message.content:
            return response.choices[0].message.content
        else:
            return "No response content from AI."
    except Exception as e:
        print(f"Error generating answer with OpenAI: {e}")
        return f"Error generating answer with OpenAI: {e}"

def responder_pregunta_rag(pregunta: str) -> str:
    if initialization_error or not openai_client_instance or (not pinecone_index_instance):
        error_detail = initialization_error or "OpenAI or Pinecone client not initialized."
        return f"Error: Services not properly initialized. Please check configuration and logs. Detail: {error_detail}"
    start_time = time.time()
    embedding = get_embedding(pregunta)
    if embedding is None:
        return "Error: No se pudo generar el embedding para la pregunta."
    contexto = buscar_contexto(embedding)
    if contexto.startswith("Error:"):
        return contexto
    respuesta_content = generar_respuesta_openai(pregunta, contexto)
    end_time = time.time()
    segundos = round(end_time - start_time, 2)
    minutos = round(segundos / 60, 2)
    return f"Respuesta:\n{respuesta_content}\n\nTiempo de respuesta: {segundos} segundos ({minutos} minutos)"

class RAGState(rx.State):
    pregunta: str = ""
    respuesta: str = ""
    is_loading: bool = False
    error_message: str = ""
    archivo_subido: str = ""
    mensaje_procesamiento: str = ""

    def _check_clients_initialized_internal(self) -> str:
        if initialization_error:
            return f"Error de inicializaci贸n: {initialization_error}. Por favor, verifica las variables de entorno (OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_INDEX_NAME) y la configuraci贸n del 铆ndice en Pinecone."
        if not openai_client_instance:
            return "El cliente de OpenAI no est谩 inicializado. Verifica OPENAI_API_KEY."
        if not pinecone_index_instance:
            return "El 铆ndice de Pinecone no est谩 inicializado. Verifica PINECONE_API_KEY, PINECONE_INDEX_NAME y que el 铆ndice exista."
        return ""

    @rx.event(background=True)
    async def generar(self):
        async with self:
            self.is_loading = True
            self.respuesta = ""
            self.error_message = ""
            if not self.pregunta.strip():
                self.respuesta = "Por favor, escribe una pregunta."
                self.is_loading = False
                yield
                return

        error_msg = self._check_clients_initialized_internal()
        if error_msg:
            async with self:
                self.error_message = error_msg
                self.respuesta = error_msg
                self.is_loading = False
                yield
            return

        generated_response = responder_pregunta_rag(self.pregunta)
        async with self:
            self.respuesta = generated_response
            if generated_response.startswith("Error:"):
                self.error_message = generated_response
            self.is_loading = False
            yield

    @rx.event(background=True)
    async def procesar_archivo(self, files: list[rx.UploadFile]):
        print("★ Evento 'procesar_archivo' disparado.")

        if not files:
            async with self:
                self.mensaje_procesamiento = "No se seleccion贸 ning煤n archivo."
                print("锔 No se recibi贸 ning煤n archivo.")
                yield
            return

        file = files[0]
        print(f" Archivo recibido: {file.filename}")

        content = await file.read()
        path = rx.get_upload_dir() / file.filename
        with open(path, "wb") as f:
            f.write(content)
        print(f" Guardado como: {path}")

        nombre = file.filename.lower()

        # Procesamiento de texto
        if nombre.endswith(".pdf"):
            import fitz
            doc = fitz.open(str(path))
            texto = "".join(p.get_text() for p in doc)
        elif nombre.endswith(".txt"):
            with open(path, "r", encoding="utf-8") as f:
                texto = f.read()
        elif nombre.endswith(".docx"):
            import docx
            doc = docx.Document(str(path))
            texto = "\n".join(p.text for p in doc.paragraphs)
        else:
            async with self:
                self.mensaje_procesamiento = "Tipo de archivo no soportado."
                yield
            return

        from langchain.text_splitter import RecursiveCharacterTextSplitter
        import uuid

        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_text(texto)
        metadatos = []

        for i, chunk in enumerate(chunks):
            try:
                embedding = get_embedding(chunk)
                metadata = {
                    "id": str(uuid.uuid4()),
                    "fuente": nombre,
                    "texto": chunk,
                    "posicion": i
                }
                pinecone_index_instance.upsert([(metadata["id"], embedding, metadata)], namespace=PINECONE_NAMESPACE)
                metadatos.append(metadata)
            except Exception as e:
                print(f"[X] Error en chunk {i}: {e}")

        async with self:
            self.mensaje_procesamiento = f"Documento '{file.filename}' procesado correctamente. Chunks: {len(metadatos)}"
            yield
